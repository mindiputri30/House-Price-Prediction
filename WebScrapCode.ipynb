{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install beautifulsoup4\n",
    "# %pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install urllib3==1.26.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from urllib.parse import urlparse, parse_qs, urlencode, urlunparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_page_url(base_url, page_number):\n",
    "#     page_url = base_url.replace('/page=1/', f'/page={page_number}/')\n",
    "#     return page_url\n",
    "\n",
    "def generate_page_url(base_url, page_number):\n",
    "    url_parts = base_url.split('=')\n",
    "    url_parts[-1] = str(page_number)\n",
    "    page_url = '='.join(url_parts)\n",
    "    return page_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data telah disimpan dalam file CSV.\n"
     ]
    }
   ],
   "source": [
    "# Scrape Link to Detail Page\n",
    "links_array = []\n",
    "dashboard_url = 'https://www.lamudi.co.id/west-nusa-tenggara/house/buy/?page=1'\n",
    "driver = webdriver.Chrome()\n",
    "wait = WebDriverWait(driver, 3)\n",
    "\n",
    "for page_number in range(1, 21):  # Ubah 20 menjadi 21 untuk memperoleh halaman 1 hingga 20\n",
    "    current_page_url = generate_page_url(dashboard_url, page_number)\n",
    "    driver.get(current_page_url)\n",
    "\n",
    "    # STATIS\n",
    "    # soup = BeautifulSoup(driver.page_source, 'html.parser')  # Menggunakan driver.page_source untuk konten halaman\n",
    "    # links = soup.find_all('a', class_='ListingCell-moreInfo-button-v2_redesign')\n",
    "    # for link in links:\n",
    "    #     print(link['href'])\n",
    "\n",
    "    # Tunggu hingga konten dinamis selesai dimuat\n",
    "    wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'ListingCell-moreInfo-button-v2_redesign')))\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    links = soup.find_all('a', class_='ListingCell-moreInfo-button-v2_redesign')\n",
    "    # for link in links:\n",
    "    #     print(link['href'])\n",
    "    # print(len(links))\n",
    "\n",
    "    # DENGAN ARRAY\n",
    "    for link in links:\n",
    "        links_array.append(link['href'])  # Tambahkan link ke dalam array\n",
    "        \n",
    "    # print(len(links))\n",
    "\n",
    "csv_file = 'data.csv'\n",
    "\n",
    "def scrape_detail(url):\n",
    "    bedrooms = bathrooms = building_size = land_size = '0'  # Default value\n",
    "\n",
    "    try:\n",
    "        bedrooms_element = soup.find('div', {'data-attr-name': 'bedrooms'}).find_next('div')\n",
    "        bedrooms = bedrooms_element.text.strip() if bedrooms_element else '0'\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        bathrooms_element = soup.find('div', {'data-attr-name': 'bathrooms'}).find_next('div')\n",
    "        bathrooms = bathrooms_element.text.strip() if bathrooms_element else '0'\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        building_size_element = soup.find('div', {'data-attr-name': 'building_size'}).find_next('div')\n",
    "        building_size = building_size_element.text.strip() if building_size_element else '0'\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        land_size_element = soup.find('div', {'data-attr-name': 'land_size'}).find_next('div')\n",
    "        land_size = land_size_element.text.strip() if land_size_element else '0'\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        price = soup.find('div', {'Title-pdp-price'}).find('span', class_='FirstPrice')\n",
    "        prices = price.text.strip() if price else '0'\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    return bedrooms, bathrooms, building_size, land_size, prices\n",
    "\n",
    "# Scrape dan simpan dalam CSV\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Jumlah Kamar Tidur', 'Jumlah Kamar Mandi', 'Luas Bangunan', 'Luas Tanah','Harga'])\n",
    "\n",
    "    for link in links_array[:600]:  # Mengambil 600 data pertama\n",
    "        driver.get(link)  # Buka halaman detail rumah berdasarkan link\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        bedrooms, bathrooms, building_size, land_size, prices = scrape_detail(soup)\n",
    "        writer.writerow([bedrooms, bathrooms, building_size, land_size, prices])\n",
    "\n",
    "print('Data telah disimpan dalam file CSV.')\n",
    "\n",
    "driver.quit()  # Tutup browser setelah selesai scraping\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
